{"cells":[{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":false,"cell_id":"cd56f0d236bf4c2b8346c4c969107a9e","deepnote_cell_type":"markdown"},"source":"# Lab 4 - Math 178, Spring 2024\n\nYou are encouraged to work in groups of up to 3 total students, but each student should make a submission on Canvas. (It's fine for everyone in the group to submit the same link.)\n\nPut the full names of everyone in your group (even if you're working alone) here. This makes grading easier.\n\n**Names**: Seth Abuhamdeh 34937889","block_group":"f74e3d8d089c49f489a72f9c14585b79"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":false,"cell_id":"7b5751e3ee5440538f51324b82503d2e","deepnote_cell_type":"markdown"},"source":"## Train an XOR network using PyTorch","block_group":"ed00ef0c22534df496d694d3efaa5c85"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":false,"cell_id":"b6d3b252f92f45db9c0cec27e2b37987","deepnote_cell_type":"markdown"},"source":"* Use PyTorch to train a neural network which produces a perfect (4 out of 4) prediction rate for XOR.  (This is similar to what you did \"by hand\" on question 2d on Homework 3.  You should not be manually setting the weights, but instead, should be using PyTorch to find weights.  Use a Binary Cross Entropy loss function.  Feel free to use a more complex Neural Network architecture than what you did by hand in the homework.  I was able to eventually get a small neural network architecture to work, but I had to re-run the code numerous times.)\n\nRecommended references:\n1. I primarily used the attached University of Washington notebook, which I downloaded from [Google Colab](https://colab.research.google.com/drive/1up-BwDyjNLISMtXKCMjNomyIOW96JlJh?usp=sharing).\n2. I personally solved this exercise before reading through the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) which will be used below.  If I had started with that tutorial, maybe I would have used a different approach.  But that tutorial is fancier than what we need here, because of the data loaders etc.\n\nComment:\n1.  The MNIST portion below is probably easier, in terms of what you need to do, but I'm putting this part first because the resulting neural network here is conceptually simpler.","block_group":"53a5de6e10ca4a9bad12f0d27dd50e57"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716322486742,"execution_millis":431,"deepnote_to_be_reexecuted":false,"cell_id":"0c2ba145d31f49d5a9e457d66918328a","deepnote_cell_type":"code"},"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim","block_group":"59b4d8ea4a7644e2b431f4c59e97a37e","execution_count":null,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/3adf5bc7-a4df-4882-8243-affcffaca2ca","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716322487177,"execution_millis":22,"deepnote_to_be_reexecuted":false,"cell_id":"96ff4335b5f749679c7c0183e0d72987","deepnote_cell_type":"code"},"source":"#build XOR dataset\nX= torch.tensor([[0,0],[0,1],[1,0],[1,1]],dtype = torch.float32)\ny = torch.tensor([[0],[1],[1],[0]], dtype = torch.float32)\n","block_group":"42e57976117b4e5ea77faeb2579fedd9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716323125087,"execution_millis":5217,"deepnote_to_be_reexecuted":false,"cell_id":"e053cb4fa5074eaf84cfca949a24146b","deepnote_cell_type":"code"},"source":"#Neural Network\nclass XORNN(nn.Module):\n    def __init__(self):\n        super(XORNN,self).__init__()\n        self.hidden1 = nn.Linear(2,4)\n        self.hidden2 = nn.Linear(4,4)\n        self.output = nn.Linear(4,1)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU()\n        nn.init.xavier_uniform_(self.hidden1.weight)\n        nn.init.xavier_uniform_(self.hidden2.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, x):\n        x = self.hidden1(x)\n        x = self.relu(x)\n        x = self.hidden2(x)\n        x = self.relu(x)\n        x = self.output(x)\n        x = self.sigmoid(x)\n        return x\n\nmodel = XORNN()\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(),lr=0.1)\n\n#train da model\nepochs = 10000\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss =  criterion(outputs, y)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 1000 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\nwith torch.no_grad():\n    predictions = model(X)\n    predicted = (predictions > 0.5).float()\n    print(f'Predictions:\\n{predicted}')\n    print(f'Actual:\\n{y}')\n","block_group":"4749b46b5e7947adb687d92b57222c2a","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1000/10000], Loss: 0.0000\nEpoch [2000/10000], Loss: 0.0000\nEpoch [3000/10000], Loss: 0.0000\nEpoch [4000/10000], Loss: 0.0000\nEpoch [5000/10000], Loss: 0.0000\nEpoch [6000/10000], Loss: 0.0000\nEpoch [7000/10000], Loss: 0.0000\nEpoch [8000/10000], Loss: 0.0000\nEpoch [9000/10000], Loss: 0.0000\nEpoch [10000/10000], Loss: 0.0000\nPredictions:\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]])\nActual:\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]])\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/2505cd5c-4b90-49a5-82bd-1e46f46251ed","content_dependencies":null},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":false,"cell_id":"b16e7cda5a544de88e38d5d9be6687eb","deepnote_cell_type":"markdown"},"source":"## Train an MNIST network using PyTorch\n\n* Adapt the code at [the PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) to train an MNIST neural network.  Adjust parameters as necessary to reach at least a 91% test accuracy.  (Be sure you're using `datasets.MNIST` rather than what's in the tutorial: `datasets.FashionMNIST`.  Most other parts of the tutorial should adapt easily.  I deleted the the GPU parts such as `if torch.cuda.is_available()` because I don't think they will work on Deepnote, but perhaps they are useful also here.)","block_group":"5731b90a5c7b4cfa8a69e48001dcc44f"},{"cell_type":"code","metadata":{"source_hash":"71c5102","execution_start":1716326113442,"execution_millis":125850,"deepnote_to_be_reexecuted":false,"cell_id":"6a316fc523ff4518b253ef826bc90d1d","deepnote_cell_type":"code"},"source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Download training data from open datasets\ntraining_data = datasets.MNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets\ntest_data = datasets.MNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\nbatch_size = 64\n\n# Create data loaders\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n# Check the data loader\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n","block_group":"96b9f0312ea04736a5464252996c4bbd","execution_count":2,"outputs":[{"name":"stdout","text":"Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\nEpoch 1\n-------------------------------\nloss: 2.306282  [   64/60000]\nloss: 0.499791  [ 6464/60000]\nloss: 0.341661  [12864/60000]\nloss: 0.145393  [19264/60000]\nloss: 0.105162  [25664/60000]\nloss: 0.150128  [32064/60000]\nloss: 0.045713  [38464/60000]\nloss: 0.113160  [44864/60000]\nloss: 0.095903  [51264/60000]\nloss: 0.073280  [57664/60000]\nTest Error: \n Accuracy: 96.8%, Avg loss: 0.108078 \n\nEpoch 2\n-------------------------------\nloss: 0.044254  [   64/60000]\nloss: 0.039624  [ 6464/60000]\nloss: 0.043773  [12864/60000]\nloss: 0.219417  [19264/60000]\nloss: 0.027558  [25664/60000]\nloss: 0.168358  [32064/60000]\nloss: 0.044276  [38464/60000]\nloss: 0.038451  [44864/60000]\nloss: 0.039650  [51264/60000]\nloss: 0.320839  [57664/60000]\nTest Error: \n Accuracy: 97.0%, Avg loss: 0.087464 \n\nEpoch 3\n-------------------------------\nloss: 0.037094  [   64/60000]\nloss: 0.006387  [ 6464/60000]\nloss: 0.027884  [12864/60000]\nloss: 0.016710  [19264/60000]\nloss: 0.026756  [25664/60000]\nloss: 0.044740  [32064/60000]\nloss: 0.022959  [38464/60000]\nloss: 0.278406  [44864/60000]\nloss: 0.106082  [51264/60000]\nloss: 0.172546  [57664/60000]\nTest Error: \n Accuracy: 97.1%, Avg loss: 0.090796 \n\nEpoch 4\n-------------------------------\nloss: 0.045233  [   64/60000]\nloss: 0.012602  [ 6464/60000]\nloss: 0.033109  [12864/60000]\nloss: 0.047145  [19264/60000]\nloss: 0.022679  [25664/60000]\nloss: 0.050522  [32064/60000]\nloss: 0.188362  [38464/60000]\nloss: 0.042413  [44864/60000]\nloss: 0.017837  [51264/60000]\nloss: 0.035994  [57664/60000]\nTest Error: \n Accuracy: 98.0%, Avg loss: 0.067748 \n\nEpoch 5\n-------------------------------\nloss: 0.007175  [   64/60000]\nloss: 0.039471  [ 6464/60000]\nloss: 0.032902  [12864/60000]\nloss: 0.057780  [19264/60000]\nloss: 0.026305  [25664/60000]\nloss: 0.020411  [32064/60000]\nloss: 0.007080  [38464/60000]\nloss: 0.010798  [44864/60000]\nloss: 0.110109  [51264/60000]\nloss: 0.000253  [57664/60000]\nTest Error: \n Accuracy: 97.5%, Avg loss: 0.087983 \n\nDone!\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/5838d7a8-44ee-4b06-8454-4ddfc859b589","content_dependencies":null},{"cell_type":"markdown","metadata":{"source_hash":"a3953636","execution_start":1715526908824,"execution_millis":117,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":false,"cell_id":"5bf696f9bee54a04be6bf1ec05ee2420","deepnote_cell_type":"markdown"},"source":"## Submission\n\n* Using the `Share` button at the top right, enable public sharing, and enable Comment privileges. Then submit the created link on Canvas.","block_group":"538dd10f752847a2a51f2372278a8688"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":false,"cell_id":"817238bd13004863af01124d2aed4658","deepnote_cell_type":"markdown"},"source":"## Possible extensions\n\n* My code for the small XOR neural network only works about one out of ten times.  Can you produce code that always works, for example, using a `while` loop or some change to the hyperparameters (not including making the hidden layer bigger)?\n* Neural networks are very prone to overfitting.  Does that happen with your MNIST code?  Can you plot a train and test error curve to demonstrate?  How can you combat overfitting, for example, using `nn.Dropout`?\n* Can you use Python (don't try to do this by hand) the decision boundary for the XOR classifier from part 1?  Here is some sample code from Math 10 for drawing decision boundaries: https://christopherdavisuci.github.io/UCI-Math-10-S23/Week8/Week8-Wednesday.html","block_group":"599d682776af487a8414b38108b9f7ab"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e1a44b28-bed7-470b-bf23-d274d60bed13' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-05-21T20:45:40.619Z"},"deepnote_notebook_id":"ab69166467094007beef514c82c5a22d","deepnote_execution_queue":[]}}